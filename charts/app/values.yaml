# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
#-- global variables, can be accessed by sub-charts.
global:
  #-- the registry where the images are stored. override during runtime for other registry at global level or individual level.
  repository: ~ # provide the repo name from where images will be sourced for example bcgo
  #-- the registry where the images are stored. override during runtime for other registry at global level or individual level. default is ghcr.io
  registry: artifacts.developer.gov.bc.ca/github-docker-remote # ghcr.io for directly streaming from github container registry or "artifacts.developer.gov.bc.ca/github-docker-remote" for artifactory, or any other registry.
  #-- the tag of the image, it can be latest, 1.0.0 etc..., or the sha256 hash
  tag: ~
  #-- turn off autoscaling for the entire suite by setting this to false. default is true.
  autoscaling: true
  #-- global secrets, can be accessed by sub-charts.
  secrets:
    enabled: true
    databasePassword: ~
    databaseUser: ~
    databaseName: ~
    keycloakSecret: ~
    keycloakRealm: ~
    keycloakClientID: ~
    keycloakAuthURL: ~
    databaseAdminPassword: ~
    persist: true
  config:
    dbName: site # if updating this value, make sure this is updated `bitnami-pg: auth:     username: site , database: site in values-pr.yaml`
    schemaName: sites
  #-- domain of the application, it is required, apps.silver.devops.gov.bc.ca for silver cluster and apps.devops.gov.bc.ca for gold cluster
  domain: "apps.silver.devops.gov.bc.ca" # it is apps.gold.devops.gov.bc.ca for gold cluster
  #-- the database Alias gives a nice way to switch to different databases, crunchy, patroni ... etc.
  databaseAlias: bitnami-pg
#-- the components of the application, backend.
backendSites:
  #-- enable or disable backend
  enabled: true
  #-- the deployment strategy, can be "Recreate" or "RollingUpdate"
  deploymentStrategy: RollingUpdate
  #-- autoscaling for the component. it is optional and is an object.
  autoscaling:
    #-- enable or disable autoscaling.
    enabled: true
    #-- the minimum number of replicas.
    minReplicas: 2
    #-- the maximum number of replicas.
    maxReplicas: 3
    #-- the target cpu utilization percentage, is from request cpu and NOT LIMIT CPU.
    targetCPUUtilizationPercentage: 80
  resources:
    limits:
      cpu: 150m
      memory: 250Mi
    requests:
      cpu: 50m
      memory: 150Mi
  #-- the service for the component. for inter namespace communication, use the service name as the hostname.
  service:
    #-- the type of the service. it can be ClusterIP, NodePort, LoadBalancer, ExternalName. ClusterIP is the default and is recommended.
    type: ClusterIP
    port: 80 # this is the service port, where it will be exposed internal to the namespace.
    targetPort: 3000 # this is container port where app listens on
  ingress:
    annotations:
      route.openshift.io/termination: "edge"
  pdb:
    enabled: true # enable it in PRODUCTION for having pod disruption budget.
    minAvailable: 1 # the minimum number of pods that must be available during the disruption budget.

frontendSiteRegistry:
  # -- enable or disable a component deployment.
  enabled: true
  # -- the deployment strategy, can be "Recreate" or "RollingUpdate"
  deploymentStrategy: RollingUpdate

  #-- autoscaling for the component. it is optional and is an object.
  autoscaling:
    #-- enable or disable autoscaling.
    enabled: true
    #-- the minimum number of replicas.
    minReplicas: 2
    #-- the maximum number of replicas.
    maxReplicas: 3
    #-- the target cpu utilization percentage, is from request cpu and NOT LIMIT CPU.
    targetCPUUtilizationPercentage: 80
  #-- the service for the component. for inter namespace communication, use the service name as the hostname.
  service:
    #-- enable or disable the service.
    enabled: true
    #-- the type of the service. it can be ClusterIP, NodePort, LoadBalancer, ExternalName. ClusterIP is the default and is recommended.
    type: ClusterIP
    #-- the ports for the service.
    ports:
      - name: http
        #-- the port for the service. the service will be accessible on this port within the namespace.
        port: 80
        #-- the container port where the application is listening on
        targetPort: 3000
        #-- the protocol for the port. it can be TCP or UDP. TCP is the default and is recommended.
        protocol: TCP
  ingress:
    annotations:
      route.openshift.io/termination: "edge"
  pdb:
    enabled: true # enable it in PRODUCTION for having pod disruption budget.
    minAvailable: 1 # the minimum number of pods that must be available during the disruption budget.
  config:
    REACT_APP_BACKEND_API: ~
    REACT_APP_AUTH_AUTHORITY: ~
    REACT_APP_AUTH_CLIENT_ID: ~
    REACT_APP_AUTH_REDIRECT_URI: ~
    REACT_APP_AUTH_LOGOUT_REDIRECT_URI: ~
    REACT_APP_AUTH_RESPONSE_TYPE: "code"
    REACT_APP_AUTH_SCOPE: "openid profile"
    REACT_APP_AUTH_FILTER_PROTOCOL_CLAIMS: "true"
    REACT_APP_AUTH_LOAD_USER_INFO: "true"
    REACT_APP_AUTH_REVOKE_TOKENS_ON_SIGNOUT: "true"
crunchy: # enable it for TEST and PROD, for PR based pipelines simply use single postgres
  enabled: true
  crunchyImage: artifacts.developer.gov.bc.ca/bcgov-docker-local/crunchy-postgres-gis:ubi8-16.2-3.3-0
  postgresVersion: 16
  postGISVersion: '3.3'
  imagePullPolicy: IfNotPresent
  # enable below to start a new crunchy cluster after disaster from a backed-up location, crunchy will choose the best place to recover from.
  # follow https://access.crunchydata.com/documentation/postgres-operator/5.2.0/tutorial/disaster-recovery/
  # Clone From Backups Stored in S3 / GCS / Azure Blob Storage
  clone:
    enabled: false
    path: ~ # provide the proper path to source the cluster. ex: /backups/cluster/version/1, if current new cluster being created, this should be current cluster version -1, ideally
  # enable this to go back to a specific timestamp in history in the current cluster.
  # follow https://access.crunchydata.com/documentation/postgres-operator/5.2.0/tutorial/disaster-recovery/
  # Perform an In-Place Point-in-time-Recovery (PITR)
  # need to fire this command `oc annotate postgrescluster pay-transparency-tools-crunchy --overwrite postgres-operator.crunchydata.com/pgbackrest-restore=repo1`
  restore:
    enabled: false
    target: ~ # 2024-03-24 17:16:00-07 this is the target timestamp to go back to in current cluster
  instances:
    name: db # high availability
    replicas: 2 # 2 or 3 for high availability in TEST and PROD.
    metadata:
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9187'
    dataVolumeClaimSpec:
      storage: 380Mi
      storageClassName: netapp-block-standard

    requests:
      cpu: 50m
      memory: 128Mi
    limits:
      cpu: 150m
      memory: 256Mi
    replicaCertCopy:
      requests:
        cpu: 1m
        memory: 32Mi
      limits:
        cpu: 50m
        memory: 64Mi

  pgBackRest:
    enabled: true
    backupPath: /backups/cluster/version
    clusterCounter: 1 # this is the number to identify what is the current counter for the cluster, each time it is cloned it should be incremented.
    image: artifacts.developer.gov.bc.ca/bcgov-docker-local/crunchy-pgbackrest:ubi8-2.49-0
    # If retention-full-type set to 'count' then the oldest backups will expire when the number of backups reach the number defined in retention
    # If retention-full-type set to 'time' then the number defined in retention will take that many days worth of full backups before expiration
    retentionFullType: count
    s3:
      retention: 7 # one weeks backup in object store.
      bucket: nrsitest
      endpoint: nrs.objectstore.gov.bc.ca
      accessKey: ~
      secretKey: ~
      fullBackupSchedule: 0 9 * * * # full backup at GMT 9
      incrementalBackupSchedule: 0 0/1 * * * # every hour
    pvc:
      retention: 1 # one day hot active backup in pvc
      retentionFullType: count
      fullBackupSchedule: 0 8 * * *
      incrementalBackupSchedule: 0 0,12 * * * # every 12 hour incremental
      volume:
        accessModes: "ReadWriteOnce"
        storage: 96Mi
        storageClassName: netapp-file-backup

    config:
      requests:
        cpu: 5m
        memory: 32Mi
      limits:
        cpu: 20m
        memory: 64Mi
    repoHost:
      requests:
        cpu: 20m
        memory: 128Mi
      limits:
        cpu: 50m
        memory: 256Mi
    sidecars:
      requests:
        cpu: 5m
        memory: 16Mi
      limits:
        cpu: 20m
        memory: 64Mi
    jobs:
      requests:
        cpu: 20m
        memory: 128Mi
      limits:
        cpu: 100m
        memory: 256Mi

  patroni:
    postgresql:
      pg_hba: "host all all 0.0.0.0/0 md5"
      parameters:
        shared_buffers: 16MB # default is 128MB; a good tuned default for shared_buffers is 25% of the memory allocated to the pod
        wal_buffers: "64kB" # this can be set to -1 to automatically set as 1/32 of shared_buffers or 64kB, whichever is larger
        min_wal_size: 32MB
        max_wal_size: 64MB # default is 1GB
        max_slot_wal_keep_size: 96MB # default is -1, allowing unlimited wal growth when replicas fall behind

  proxy:
    enabled: true
    pgBouncer:
      image: # it's not necessary to specify an image as the images specified in the Crunchy Postgres Operator will be pulled by default
      replicas: 1
      requests:
        cpu: 5m
        memory: 32Mi
      limits:
        cpu: 20m
        memory: 64Mi

  # Postgres Cluster resource values:
  pgmonitor:
    enabled: false
    exporter:
      image: # it's not necessary to specify an image as the images specified in the Crunchy Postgres Operator will be pulled by default
      requests:
        cpu: 1m
        memory: 16Mi
      limits:
        cpu: 35m
        memory: 32Mi

bitnami-pg:
  enabled: false

